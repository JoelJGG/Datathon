{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QXvqXpaVip-M"
      },
      "source": [
        "# Schneider challenge"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "srbzFxkhij5H"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "bxCCfmv9icSH"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torch.utils.data import TensorDataset, random_split\n",
        "\n",
        "def process_dataset(csv):\n",
        "  df = pd.read_csv(csv)\n",
        "\n",
        "  cols_to_use = list(range(1,16))\n",
        "  X = df.iloc[:, cols_to_use].values.astype(np.float32)\n",
        "  y = df.iloc[:, 16].values  # última columna es el target\n",
        "\n",
        "  features_tensor = torch.from_numpy(X)\n",
        "  labels_tensor = torch.from_numpy(y)\n",
        "  full_dataset = TensorDataset(features_tensor, labels_tensor)\n",
        "\n",
        "  train_data, test_data = create_dataset(full_dataset)\n",
        "  return train_data, test_data\n",
        "\n",
        "def create_dataset(full_dataset):\n",
        "    train_dataset, test_dataset = random_split(full_dataset, [0.8,0.2])\n",
        "    return train_dataset, test_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7WsHOjxQowf"
      },
      "source": [
        "Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "Sz21z1vmx1xm"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def create_dataloader(train_dataset, test_dataset, BATCH_SIZE):\n",
        "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE,shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE,shuffle=False)\n",
        "    return train_loader, test_loader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "de7b7mF8it_V"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "4ojW9SRmiveE"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class SchneiderModel(nn.Module):\n",
        "    def __init__(self, inputs=15, outputs=2):\n",
        "        super().__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Linear(inputs, 1500),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Linear(1500, 750),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Linear(750, 375),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Linear(375, 190),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Linear(190, 95),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Linear(95, 30),\n",
        "            nn.Tanh(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(30, outputs)\n",
        "        )\n",
        "\n",
        "    def forward(self, X):\n",
        "        return self.features(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QChjYrPlFUEF"
      },
      "source": [
        "SHAP NN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "NWeCmeStFSy2"
      },
      "outputs": [],
      "source": [
        "\n",
        "def shap_nn_model(model, X_train, X_test, device,\n",
        "                  feature_names=None,\n",
        "                  background_size=300,\n",
        "                  sample_size=300):\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    X_train = np.array(X_train, dtype=np.float32)\n",
        "    X_test  = np.array(X_test, dtype=np.float32)\n",
        "\n",
        "    n_train = X_train.shape[0]\n",
        "    n_test  = X_test.shape[0]\n",
        "\n",
        "    background_size = min(background_size, n_train)\n",
        "    idx_bg = np.random.choice(n_train, size=background_size, replace=False)\n",
        "    X_bg_np = X_train[idx_bg]\n",
        "\n",
        "    sample_size = min(sample_size, n_test)\n",
        "    idx_explain = np.random.choice(n_test, size=sample_size, replace=False)\n",
        "    X_explain_np = X_test[idx_explain]\n",
        "\n",
        "    if feature_names is None:\n",
        "        feature_names = [f\"feature_{i}\" for i in range(X_train.shape[1])]\n",
        "\n",
        "    X_bg = torch.from_numpy(X_bg_np).to(device)\n",
        "    X_explain = torch.from_numpy(X_explain_np).to(device)\n",
        "\n",
        "    explainer = shap.DeepExplainer(model, X_bg)\n",
        "    shap_values = explainer.shap_values(X_explain)\n",
        "    expected_value = explainer.expected_value\n",
        "\n",
        "    if isinstance(shap_values, list):\n",
        "        class_idx = 1 if len(shap_values) > 1 else 0\n",
        "        sv = np.array(shap_values[class_idx])\n",
        "        if isinstance(expected_value, (list, np.ndarray)):\n",
        "            base_val = float(np.array(expected_value)[class_idx])\n",
        "        else:\n",
        "            base_val = float(expected_value)\n",
        "    else:\n",
        "        sv = np.array(shap_values)\n",
        "        if sv.ndim == 3:\n",
        "            class_idx = 1 if sv.shape[-1] > 1 else 0\n",
        "            sv = sv[..., class_idx]\n",
        "            if isinstance(expected_value, (list, np.ndarray)):\n",
        "                base_val = float(np.array(expected_value)[class_idx])\n",
        "            else:\n",
        "                base_val = float(expected_value)\n",
        "        else:\n",
        "            if isinstance(expected_value, (list, np.ndarray)):\n",
        "                class_idx = 1 if len(expected_value) > 1 else 0\n",
        "                base_val = float(np.array(expected_value)[class_idx])\n",
        "            else:\n",
        "                base_val = float(expected_value)\n",
        "\n",
        "\n",
        "    print(\"\\n[NN] SHAP - Importancia media de las features (|SHAP|)\")\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    shap.summary_plot(\n",
        "        sv,\n",
        "        X_explain_np,\n",
        "        feature_names=feature_names,\n",
        "        plot_type=\"bar\",\n",
        "        show=True\n",
        "    )\n",
        "\n",
        "    print(\"\\n[NN] SHAP - Beeswarm\")\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    shap.summary_plot(\n",
        "        sv,\n",
        "        X_explain_np,\n",
        "        feature_names=feature_names,\n",
        "        show=True\n",
        "    )\n",
        "\n",
        "    print(\"\\n[NN] SHAP - Waterfall ejemplo 0 (clase positiva)\")\n",
        "    exp = shap.Explanation(\n",
        "        values=sv[0],\n",
        "        base_values=base_val,\n",
        "        data=X_explain_np[0],\n",
        "        feature_names=feature_names\n",
        "    )\n",
        "    shap.plots.waterfall(exp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Md_bb2JTil-N"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "j5cLS_WWibU-"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "\n",
        "def train(dataloader,model,loss_fn,optimizer,device, EPOCHS):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        train_bar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{EPOCHS} [Train]\", leave=True)\n",
        "        for X, y in train_bar:\n",
        "            X,y = X.to(device), y.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            pred = model(X)\n",
        "            loss = loss_fn(pred,y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item() * X.size(0)\n",
        "            _, predicted = torch.max(pred, 1)\n",
        "            correct += (predicted == y).sum().item()\n",
        "            total += y.size(0)\n",
        "\n",
        "            train_bar.set_postfix(loss=running_loss/total, accuracy=100. * correct/total)\n",
        "\n",
        "        tqdm.write(f\"Epoch [{epoch+1}/{EPOCHS}] \"\n",
        "                f\"Train Loss: {running_loss/total:.4f} | Train Acc: {100. * correct/total:.4f} \")\n",
        "    torch.save(model.state_dict(), \"schneider_model.pth\")\n",
        "    tqdm.write(f\"Training complete. Model saved as schneider_model.pth\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0yQRzE1inx1"
      },
      "source": [
        "## Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "nirMMfnqiYrD"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "def test(dataloader, model, loss_fn, device):\n",
        "    model.eval()\n",
        "    test_loss = 0.0\n",
        "    test_correct = 0\n",
        "    test_total = 0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            X = X.to(device)\n",
        "            y = y.to(device)\n",
        "            outputs = model(X)\n",
        "            loss = loss_fn(outputs, y)\n",
        "            test_loss += loss.item() * X.size(0)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            test_correct += (predicted == y).sum().item()\n",
        "            test_total += y.size(0)\n",
        "            all_preds.extend(predicted.cpu().numpy())\n",
        "            all_labels.extend(y.cpu().numpy())\n",
        "\n",
        "    test_loss = test_loss / test_total\n",
        "    test_acc = test_correct / test_total\n",
        "\n",
        "    print(f\"Test Loss: {test_loss:.4f} | Test Accuracy: {test_acc:.4f}\")\n",
        "\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(all_labels, all_preds, target_names=['failed', 'succeed']))\n",
        "\n",
        "    import matplotlib.pyplot as plt\n",
        "    from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "    cm = confusion_matrix(all_labels, all_preds)\n",
        "\n",
        "    class_names = ['failed', 'succeed']\n",
        "\n",
        "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)\n",
        "    fig, ax = plt.subplots(figsize=(6, 6))\n",
        "    disp.plot(cmap='Blues', ax=ax, colorbar=False)\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DUERLWjChY1Z"
      },
      "source": [
        "## Decision Tree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "Hnt-YA8IhY1a"
      },
      "outputs": [],
      "source": [
        "from sklearn import tree\n",
        "from sklearn.metrics import ConfusionMatrixDisplay, accuracy_score, classification_report\n",
        "\n",
        "def decision_tree_model(X_train, y_train, X_test, y_test, feature_names=None):\n",
        "    classifier = tree.DecisionTreeClassifier(\n",
        "        max_depth=20,\n",
        "        min_samples_split=15,\n",
        "        min_samples_leaf=10,\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    classifier.fit(X_train, y_train)\n",
        "    y_pred = classifier.predict(X_test)\n",
        "\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    print(f\"Decision Tree Accuracy: {acc:.4f}\")\n",
        "\n",
        "    print(\"\\nDecision Tree Classification Report:\")\n",
        "    print(classification_report(\n",
        "        y_test, y_pred,\n",
        "        target_names=['failed', 'succeed'],\n",
        "        zero_division=0\n",
        "    ))\n",
        "\n",
        "    # Visualizar árbol\n",
        "    plt.figure(figsize=(20, 10))\n",
        "    if feature_names is None:\n",
        "        feature_names_plot = [f'Feature_{i}' for i in range(X_train.shape[1])]\n",
        "    else:\n",
        "        feature_names_plot = feature_names\n",
        "\n",
        "    tree.plot_tree(\n",
        "        classifier,\n",
        "        filled=True,\n",
        "        feature_names=feature_names_plot,\n",
        "        class_names=['failed', 'succeed'],\n",
        "        rounded=True\n",
        "    )\n",
        "    plt.show()\n",
        "\n",
        "    # Matriz de confusión\n",
        "    ConfusionMatrixDisplay.from_estimator(\n",
        "        classifier,\n",
        "        X_test,\n",
        "        y_test,\n",
        "        display_labels=['failed', 'succeed']\n",
        "    )\n",
        "    plt.show()\n",
        "\n",
        "    return classifier\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "4qO2CpjJXO80"
      },
      "outputs": [],
      "source": [
        "from lime.lime_tabular import LimeTabularExplainer\n",
        "\n",
        "def lime_explain_sklearn_model(explainer, model, X_test, feature_names, class_names,\n",
        "                               instance_index=0, num_features=10):\n",
        "    \"\"\"\n",
        "    Genera una explicación LIME para un modelo sklearn (DecisionTree, RandomForest, etc.)\n",
        "    sobre una instancia concreta de X_test.\n",
        "    \"\"\"\n",
        "    instance = X_test[instance_index]\n",
        "\n",
        "    predict_fn = model.predict_proba\n",
        "\n",
        "    exp = explainer.explain_instance(\n",
        "        data_row=instance,\n",
        "        predict_fn=predict_fn,\n",
        "        num_features=num_features,\n",
        "        top_labels=1\n",
        "    )\n",
        "\n",
        "    # Mostrar en consola y devolver el objeto exp por si quieres guardarlo o plotear\n",
        "    print(f\"\\nLIME para sklearn model en índice {instance_index}\")\n",
        "    print(exp.as_list(label=1))  # contribuciones hacia la clase 1 ('succeed', por ejemplo)\n",
        "\n",
        "    # Si quieres visualizar como gráfico:\n",
        "    fig = exp.as_pyplot_figure(label=1)\n",
        "    import matplotlib.pyplot as plt\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return exp\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "HoxAMFDPXNS2"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def make_nn_predict_proba(model, device):\n",
        "    \"\"\"\n",
        "    Devuelve una función tipo predict_proba(X) para usar con LIME\n",
        "    a partir de un modelo PyTorch entrenado.\n",
        "    \"\"\"\n",
        "\n",
        "    def predict_proba(X):\n",
        "        \"\"\"\n",
        "        X: np.ndarray de forma (n_samples, n_features)\n",
        "        return: np.ndarray (n_samples, n_classes) con probabilidades\n",
        "        \"\"\"\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            X_tensor = torch.from_numpy(X.astype(np.float32)).to(device)\n",
        "            logits = model(X_tensor)             # (n_samples, n_classes)\n",
        "            probs = F.softmax(logits, dim=1)     # convertir a probabilidades\n",
        "            return probs.cpu().numpy()\n",
        "\n",
        "    return predict_proba\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "U-_ttvy4XLkw"
      },
      "outputs": [],
      "source": [
        "from lime.lime_tabular import LimeTabularExplainer\n",
        "\n",
        "def lime_explain_nn_model(explainer, model, device, X_test, feature_names, class_names,\n",
        "                          instance_index=0, num_features=10):\n",
        "    \"\"\"\n",
        "    Genera una explicación LIME para la red neuronal SchneiderModel\n",
        "    sobre una instancia concreta de X_test.\n",
        "    \"\"\"\n",
        "    instance = X_test[instance_index]\n",
        "\n",
        "    predict_fn = make_nn_predict_proba(model, device)\n",
        "\n",
        "    exp = explainer.explain_instance(\n",
        "        data_row=instance,\n",
        "        predict_fn=predict_fn,\n",
        "        num_features=num_features,\n",
        "        top_labels=1\n",
        "    )\n",
        "\n",
        "    print(f\"\\nLIME para NN en índice {instance_index}\")\n",
        "    print(exp.as_list(label=1))  # contribuciones hacia la clase 1 ('succeed')\n",
        "\n",
        "    import matplotlib.pyplot as plt\n",
        "    fig = exp.as_pyplot_figure(label=1)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return exp\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IY3CJBpYjJRc"
      },
      "source": [
        "Decision Tree SHAP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "DtgDw7vqhqrA"
      },
      "outputs": [],
      "source": [
        "import shap\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def shap_tree_model(classifier, X_test, feature_names=None):\n",
        "    if feature_names is None:\n",
        "        feature_names = [f\"feature_{i}\" for i in range(X_test.shape[1])]\n",
        "\n",
        "    explainer = shap.TreeExplainer(classifier)\n",
        "    shap_values = explainer.shap_values(X_test)\n",
        "    base_value = explainer.expected_value\n",
        "\n",
        "    # Normalizar formas (lista vs array, 2D vs 3D)\n",
        "    if isinstance(shap_values, list):\n",
        "        class_idx = 1 if len(shap_values) > 1 else 0\n",
        "        sv = np.array(shap_values[class_idx])\n",
        "        if isinstance(base_value, (list, np.ndarray)):\n",
        "            base_val = float(np.array(base_value)[class_idx])\n",
        "        else:\n",
        "            base_val = float(base_value)\n",
        "    else:\n",
        "        sv = np.array(shap_values)\n",
        "        if sv.ndim == 3:\n",
        "            class_idx = 1 if sv.shape[-1] > 1 else 0\n",
        "            sv = sv[..., class_idx]\n",
        "            if isinstance(base_value, (list, np.ndarray)):\n",
        "                base_val = float(np.array(base_value)[class_idx])\n",
        "            else:\n",
        "                base_val = float(base_value)\n",
        "        else:\n",
        "            base_val = float(np.array(base_value))\n",
        "\n",
        "    #  1) Barras: ranking de importancia\n",
        "    print(\"Importancia media de las features (|SHAP|):\")\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    shap.summary_plot(\n",
        "        sv, X_test,\n",
        "        feature_names=feature_names,\n",
        "        plot_type=\"bar\",\n",
        "        show=True\n",
        "    )\n",
        "\n",
        "    #  2) Beeswarm\n",
        "    print(\"Beeswarm (distribución de efectos por feature):\")\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    shap.summary_plot(\n",
        "        sv, X_test,\n",
        "        feature_names=feature_names,\n",
        "        show=True\n",
        "    )\n",
        "\n",
        "    #  3 Waterfall\n",
        "    i = 0\n",
        "    exp = shap.Explanation(\n",
        "        values=sv[i],\n",
        "        base_values=base_val,\n",
        "        data=X_test[i],\n",
        "        feature_names=feature_names\n",
        "    )\n",
        "    shap.plots.waterfall(exp)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZun_Tqt6qgV"
      },
      "source": [
        "Random Forest SHAP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "qdierTBWjG43"
      },
      "outputs": [],
      "source": [
        "import shap\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def shap_random_forest_fast(classifier, X_test, feature_names=None, sample_size=300):\n",
        "    if feature_names is None:\n",
        "        feature_names = [f\"feature_{i}\" for i in range(X_test.shape[1])]\n",
        "\n",
        "    # Seleccionar subset para SHAP (300 muestras)\n",
        "    idx = np.random.choice(len(X_test), size=min(sample_size, len(X_test)), replace=False)\n",
        "    X_shap = X_test[idx]\n",
        "\n",
        "    # Explainer aproximado\n",
        "    explainer = shap.TreeExplainer(\n",
        "        classifier,\n",
        "        feature_perturbation=\"tree_path_dependent\"\n",
        "    )\n",
        "\n",
        "    shap_values = explainer.shap_values(X_shap)\n",
        "\n",
        "    if isinstance(shap_values, list):\n",
        "        sv = shap_values[1]\n",
        "    else:\n",
        "        sv = shap_values[..., 1]\n",
        "\n",
        "    # Gráfico de barras\n",
        "    shap.summary_plot(sv, X_shap, feature_names=feature_names, plot_type=\"bar\")\n",
        "\n",
        "    #  Beeswarm\n",
        "    print(\"Beeswarm plot de SHAP - Random Forest:\")\n",
        "    shap.summary_plot(\n",
        "        sv,\n",
        "        X_shap,\n",
        "        feature_names=feature_names\n",
        "    )\n",
        "\n",
        "    # Waterfall\n",
        "    i = 0\n",
        "    print(f\"Waterfall plot para la muestra {i} - Random Forest:\")\n",
        "    exp = shap.Explanation(\n",
        "        values=sv[i],\n",
        "        base_values=base_val,\n",
        "        data=X_shap[i],\n",
        "        feature_names=feature_names\n",
        "    )\n",
        "    shap.plots.waterfall(exp, max_display=10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FbFm_FKNhY1a"
      },
      "source": [
        "## Random Forest Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "0Lyd_BeYhY1a"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import ConfusionMatrixDisplay, accuracy_score, classification_report\n",
        "\n",
        "def random_forest(X_train, y_train, X_test, y_test):\n",
        "    classifier = RandomForestClassifier(n_estimators=200, max_depth=25, min_samples_split=10, min_samples_leaf=5, random_state=42)\n",
        "    classifier.fit(X_train, y_train)\n",
        "    y_pred = classifier.predict(X_test)\n",
        "\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    print(f\"Random Forest Accuracy: {acc:.4f}\")\n",
        "\n",
        "    print(\"\\nRandom Forest Classification Report:\")\n",
        "    print(classification_report(y_test, y_pred, target_names=['failed', 'succeed'], zero_division=0))\n",
        "\n",
        "    ConfusionMatrixDisplay.from_estimator(classifier, X_test, y_test, display_labels=['failed', 'succeed'])\n",
        "    plt.show()\n",
        "    return classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VM7Ge9yYhY1a"
      },
      "source": [
        "## BEST FEATURES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        },
        "id": "7jllME3GhY1a",
        "outputId": "50e28b8c-fd01-42e6-a125-a179582b06c1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n    # Feature importance\\n    feature_names = ['product_A_sold_in_the_past','product_B_sold_in_the_past','product_A_recommended','product_A','product_C','product_D','cust_hitrate','cust_interactions','cust_contracts','opp_month','opp_old','competitor_Z','competitor_X','competitor_Y','cust_in_iberia']\\n    importances = classifier.feature_importances_\\n    indices = np.argsort(importances)[::-1][:10]\\n    plt.figure(figsize=(10, 6))\\n    plt.title('Top 10 Feature Importance')\\n    plt.barh(range(10), importances[indices])\\n    plt.yticks(range(10), [feature_names[i] for i in indices], rotation=45)\\n    plt.gca().invert_yaxis()\\n    plt.show()\\n  \""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 81
        }
      ],
      "source": [
        "'''\n",
        "    # Feature importance\n",
        "    feature_names = ['product_A_sold_in_the_past','product_B_sold_in_the_past','product_A_recommended','product_A','product_C','product_D','cust_hitrate','cust_interactions','cust_contracts','opp_month','opp_old','competitor_Z','competitor_X','competitor_Y','cust_in_iberia']\n",
        "    importances = classifier.feature_importances_\n",
        "    indices = np.argsort(importances)[::-1][:10]\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.title('Top 10 Feature Importance')\n",
        "    plt.barh(range(10), importances[indices])\n",
        "    plt.yticks(range(10), [feature_names[i] for i in indices], rotation=45)\n",
        "    plt.gca().invert_yaxis()\n",
        "    plt.show()\n",
        "  '''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XxWz7Q03hY1b"
      },
      "source": [
        "## Select K-best"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "3nYmxnk9hY1b"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_selection import SelectKBest\n",
        "\n",
        "def select_k_best_features(X_train, y_train, X_test, y_test):\n",
        "    X_new = SelectKBest(k=7)\n",
        "    X_new.fit(X_train, y_train)\n",
        "    X_train2 = X_new.transform(X_train)\n",
        "    X_test2 = X_new.transform(X_test)\n",
        "    selected_feature_mask = X_new.get_support()\n",
        "\n",
        "    return X_train2, y_train, X_test2, y_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7MrM9RME6LOW"
      },
      "source": [
        "## LIME"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "z6FES0LAaOC9"
      },
      "outputs": [],
      "source": [
        "def create_lime_explainer(X_train, feature_names, class_names):\n",
        "    n_features = X_train.shape[1]\n",
        "    if len(feature_names) != n_features:\n",
        "        print(\"⚠ Ajustando feature_names: X_train tiene\",\n",
        "              n_features, \"columnas pero feature_names tiene\",\n",
        "              len(feature_names))\n",
        "        feature_names = [f\"feature_{i}\" for i in range(n_features)]\n",
        "\n",
        "    explainer = LimeTabularExplainer(\n",
        "        training_data=X_train,\n",
        "        feature_names=feature_names,\n",
        "        class_names=class_names,\n",
        "        mode=\"classification\",\n",
        "        discretize_continuous=True\n",
        "    )\n",
        "    return explainer\n",
        "\n",
        "\n",
        "def make_nn_predict_proba(model, device):\n",
        "    def predict_proba(X):\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            X_tensor = torch.from_numpy(X.astype(np.float32)).to(device)\n",
        "            logits = model(X_tensor)\n",
        "            probs = F.softmax(logits, dim=1)\n",
        "            return probs.cpu().numpy()\n",
        "    return predict_proba\n",
        "\n",
        "\n",
        "def lime_explain_nn_model(explainer, model, device, X_test,\n",
        "                          instance_index=0, num_features=10):\n",
        "    instance = X_test[instance_index]\n",
        "    predict_fn = make_nn_predict_proba(model, device)\n",
        "\n",
        "    exp = explainer.explain_instance(\n",
        "        data_row=instance,\n",
        "        predict_fn=predict_fn,\n",
        "        num_features=num_features,\n",
        "        top_labels=1\n",
        "    )\n",
        "\n",
        "    print(f\"\\nLIME para NN en índice {instance_index}\")\n",
        "    print(exp.as_list(label=1))\n",
        "\n",
        "    fig = exp.as_pyplot_figure(label=1)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    return exp\n",
        "\n",
        "\n",
        "def lime_explain_sklearn_model(explainer, model, X_test,\n",
        "                               instance_index=0, num_features=10):\n",
        "    instance = X_test[instance_index]\n",
        "    predict_fn = model.predict_proba\n",
        "\n",
        "    exp = explainer.explain_instance(\n",
        "        data_row=instance,\n",
        "        predict_fn=predict_fn,\n",
        "        num_features=num_features,\n",
        "        top_labels=2\n",
        "    )\n",
        "\n",
        "    labels = exp.available_labels()\n",
        "    print(\"Etiquetas disponibles en LIME (sklearn):\", labels)\n",
        "    if 1 in labels:\n",
        "        label_to_use = 1\n",
        "    else:\n",
        "        label_to_use = labels[0]\n",
        "\n",
        "    print(f\"\\nLIME para modelo sklearn en índice {instance_index}, label={label_to_use}\")\n",
        "    print(exp.as_list(label=label_to_use))\n",
        "\n",
        "    fig = exp.as_pyplot_figure(label=label_to_use)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    return exp\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A5Lnl_ce7HVt"
      },
      "source": [
        "## Permutation Importance Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "EvA4aMpM7H4a"
      },
      "outputs": [],
      "source": [
        "from sklearn.inspection import permutation_importance\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def permutation_importance_analysis(rf_model, X_test, y_test, feature_names):\n",
        "    # Use a held-out test set (X_test, y_test)\n",
        "    result = permutation_importance(\n",
        "        rf_model, X_test, y_test,\n",
        "        n_repeats=20\n",
        "    )\n",
        "\n",
        "    for i in result.importances_mean.argsort()[::-1]:\n",
        "        print(f\"{feature_names[i]}: {result.importances_mean[i]:.4f} +/- {result.importances_std[i]:.4f}\")\n",
        "\n",
        "    # --- Plotting ---\n",
        "    sorted_idx = result.importances_mean.argsort()\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.barh(\n",
        "        range(len(sorted_idx)),\n",
        "        result.importances_mean[sorted_idx],\n",
        "        xerr=result.importances_std[sorted_idx],\n",
        "        align=\"center\"\n",
        "    )\n",
        "    plt.yticks(range(len(sorted_idx)), [feature_names[i] for i in sorted_idx])\n",
        "    plt.xlabel(\"Permutation Importance\")\n",
        "    plt.title(\"Feature Importance (Permutation)\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CtGnxegByRmG"
      },
      "source": [
        "## Main Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wyTI8_GjyR-r",
        "outputId": "f1fcfc2d-e65d-4b0e-bd60-1c0dbb4228ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train shape: (28720, 15)\n",
            "len(original_feature_names): 15\n",
            "\n",
            "======================\n",
            "MODELO 1: RED NEURONAL\n",
            "======================\n",
            "\n",
            "[NN] Entrenando modelo...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/35 [Train]: 100%|██████████| 113/113 [00:09<00:00, 12.31it/s, accuracy=64, loss=0.635]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/35] Train Loss: 0.6352 | Train Acc: 63.9519 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/35 [Train]: 100%|██████████| 113/113 [00:07<00:00, 14.58it/s, accuracy=64.5, loss=0.63]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [2/35] Train Loss: 0.6299 | Train Acc: 64.5474 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/35 [Train]: 100%|██████████| 113/113 [00:06<00:00, 16.34it/s, accuracy=64.9, loss=0.626]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [3/35] Train Loss: 0.6261 | Train Acc: 64.8549 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/35 [Train]: 100%|██████████| 113/113 [00:07<00:00, 14.32it/s, accuracy=65.1, loss=0.623]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [4/35] Train Loss: 0.6233 | Train Acc: 65.1419 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/35 [Train]: 100%|██████████| 113/113 [00:06<00:00, 16.38it/s, accuracy=65.4, loss=0.62]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [5/35] Train Loss: 0.6204 | Train Acc: 65.4130 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/35 [Train]: 100%|██████████| 113/113 [00:08<00:00, 14.11it/s, accuracy=65.7, loss=0.617]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [6/35] Train Loss: 0.6173 | Train Acc: 65.6726 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/35 [Train]: 100%|██████████| 113/113 [00:07<00:00, 16.12it/s, accuracy=65.9, loss=0.614]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [7/35] Train Loss: 0.6142 | Train Acc: 65.9486 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/35 [Train]: 100%|██████████| 113/113 [00:07<00:00, 14.45it/s, accuracy=66.2, loss=0.611]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [8/35] Train Loss: 0.6112 | Train Acc: 66.2356 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9/35 [Train]: 100%|██████████| 113/113 [00:07<00:00, 14.20it/s, accuracy=66.5, loss=0.608]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [9/35] Train Loss: 0.6079 | Train Acc: 66.5440 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10/35 [Train]: 100%|██████████| 113/113 [00:06<00:00, 16.20it/s, accuracy=66.9, loss=0.604]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/35] Train Loss: 0.6044 | Train Acc: 66.8583 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 11/35 [Train]: 100%|██████████| 113/113 [00:08<00:00, 13.74it/s, accuracy=67.2, loss=0.601]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [11/35] Train Loss: 0.6009 | Train Acc: 67.1547 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 12/35 [Train]: 100%|██████████| 113/113 [00:06<00:00, 16.30it/s, accuracy=67.5, loss=0.597]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [12/35] Train Loss: 0.5974 | Train Acc: 67.4513 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 13/35 [Train]: 100%|██████████| 113/113 [00:09<00:00, 12.25it/s, accuracy=67.7, loss=0.594]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [13/35] Train Loss: 0.5939 | Train Acc: 67.7322 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 14/35 [Train]: 100%|██████████| 113/113 [00:08<00:00, 12.94it/s, accuracy=68, loss=0.59]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [14/35] Train Loss: 0.5902 | Train Acc: 68.0193 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 15/35 [Train]: 100%|██████████| 113/113 [00:07<00:00, 15.67it/s, accuracy=68.3, loss=0.586]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [15/35] Train Loss: 0.5863 | Train Acc: 68.3254 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 16/35 [Train]: 100%|██████████| 113/113 [00:07<00:00, 14.15it/s, accuracy=68.6, loss=0.583]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [16/35] Train Loss: 0.5827 | Train Acc: 68.5968 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 17/35 [Train]: 100%|██████████| 113/113 [00:07<00:00, 16.04it/s, accuracy=68.9, loss=0.579]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [17/35] Train Loss: 0.5789 | Train Acc: 68.8745 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 18/35 [Train]: 100%|██████████| 113/113 [00:08<00:00, 14.08it/s, accuracy=69.1, loss=0.575]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [18/35] Train Loss: 0.5752 | Train Acc: 69.1367 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 19/35 [Train]: 100%|██████████| 113/113 [00:07<00:00, 14.31it/s, accuracy=69.4, loss=0.571]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [19/35] Train Loss: 0.5713 | Train Acc: 69.4209 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 20/35 [Train]: 100%|██████████| 113/113 [00:06<00:00, 16.27it/s, accuracy=69.7, loss=0.567]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [20/35] Train Loss: 0.5675 | Train Acc: 69.6908 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 21/35 [Train]: 100%|██████████| 113/113 [00:07<00:00, 14.21it/s, accuracy=70, loss=0.564]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [21/35] Train Loss: 0.5637 | Train Acc: 69.9637 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 22/35 [Train]: 100%|██████████| 113/113 [00:06<00:00, 16.37it/s, accuracy=70.2, loss=0.56]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [22/35] Train Loss: 0.5598 | Train Acc: 70.2266 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 23/35 [Train]: 100%|██████████| 113/113 [00:07<00:00, 14.30it/s, accuracy=70.5, loss=0.556]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [23/35] Train Loss: 0.5559 | Train Acc: 70.4912 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 24/35 [Train]: 100%|██████████| 113/113 [00:06<00:00, 16.19it/s, accuracy=70.7, loss=0.552]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [24/35] Train Loss: 0.5521 | Train Acc: 70.7347 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 25/35 [Train]: 100%|██████████| 113/113 [00:07<00:00, 14.39it/s, accuracy=71, loss=0.548]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [25/35] Train Loss: 0.5484 | Train Acc: 70.9795 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 26/35 [Train]:  97%|█████████▋| 110/113 [00:07<00:00, 10.80it/s, accuracy=71.2, loss=0.545]"
          ]
        }
      ],
      "source": [
        "\n",
        "def main():\n",
        "    # 1) Nombres de features y clases\n",
        "    original_feature_names = [\n",
        "        'product_A_sold_in_the_past',\n",
        "        'product_B_sold_in_the_past',\n",
        "        'product_A_recommended',\n",
        "        'product_A',\n",
        "        'product_C',\n",
        "        'product_D',\n",
        "        'cust_hitrate',\n",
        "        'cust_interactions',\n",
        "        'cust_contracts',\n",
        "        'opp_month',\n",
        "        'opp_old',\n",
        "        'competitor_Z',\n",
        "        'competitor_X',\n",
        "        'competitor_Y',\n",
        "        'cust_in_iberia'\n",
        "    ]\n",
        "    class_names = ['failed', 'succeed']\n",
        "\n",
        "    # 2) Hiperparámetros y device\n",
        "    BATCH_SIZE = 256\n",
        "    LEARNING_RATE = 0.0008\n",
        "    EPOCHS = 35\n",
        "    torch.manual_seed(42)\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    # 3) Dataset y dataloaders (comunes para todos los modelos)\n",
        "    train_dataset, test_dataset = process_dataset(\"dataset.csv\")\n",
        "    train_dataloader, test_dataloader = create_dataloader(\n",
        "        train_dataset, test_dataset, BATCH_SIZE\n",
        "    )\n",
        "\n",
        "    # 4) Datos en numpy para sklearn / LIME / SHAP\n",
        "    X_train = torch.stack([X for X, _ in train_dataset]).cpu().numpy()\n",
        "    y_train = torch.stack([y for _, y in train_dataset]).cpu().numpy().astype(int)\n",
        "\n",
        "    X_test = torch.stack([X for X, y in test_dataset]).cpu().numpy()\n",
        "    y_test = torch.stack([y for X, y in test_dataset]).cpu().numpy().astype(int)\n",
        "\n",
        "    print(\"X_train shape:\", X_train.shape)\n",
        "    print(\"len(original_feature_names):\", len(original_feature_names))\n",
        "\n",
        "    # 5) Explainer LIME común (lo reutilizamos para los 3 modelos)\n",
        "    lime_explainer = create_lime_explainer(\n",
        "        X_train=X_train,\n",
        "        feature_names=original_feature_names,\n",
        "        class_names=class_names\n",
        "    )\n",
        "\n",
        "    #NN\n",
        "    print(\"\\n======================\")\n",
        "    print(\"MODELO 1: RED NEURONAL\")\n",
        "    print(\"======================\")\n",
        "\n",
        "    # 1. Entrenar NN\n",
        "    model = SchneiderModel().to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "    print(\"\\n[NN] Entrenando modelo...\")\n",
        "    train(train_dataloader, model, loss_fn, optimizer, device, EPOCHS)\n",
        "    test(test_dataloader, model, loss_fn, device)\n",
        "    # SHAP NN\n",
        "    print(\"\\n[NN] SHAP\")\n",
        "    shap_nn_model(\n",
        "      model,\n",
        "      X_train,\n",
        "      X_test,\n",
        "      device=device,\n",
        "     feature_names=original_feature_names\n",
        "    )\n",
        "\n",
        "    # LIME para NN\n",
        "    print(\"\\n[NN] LIME\")\n",
        "    lime_explain_nn_model(\n",
        "        explainer=lime_explainer,\n",
        "        model=model,\n",
        "        device=device,\n",
        "        X_test=X_test,\n",
        "        instance_index=0,\n",
        "        num_features=10\n",
        "    )\n",
        "\n",
        "    # Decision Tree\n",
        "    print(\"\\n==========================\")\n",
        "    print(\"MODELO 2: DECISION TREE\")\n",
        "    print(\"==========================\")\n",
        "\n",
        "    # Entrenar Decision Tree\n",
        "    print(\"\\n[DT] Entrenando modelo...\")\n",
        "    tree_clf = decision_tree_model(\n",
        "        X_train, y_train, X_test, y_test,\n",
        "        feature_names=original_feature_names\n",
        "    )\n",
        "\n",
        "    # SHAP para Decision Tree\n",
        "    print(\"\\n[DT] SHAP\")\n",
        "    shap_tree_model(tree_clf, X_test, feature_names=original_feature_names)\n",
        "\n",
        "    # LIME para Decision Tree\n",
        "    print(\"\\n[DT] LIME\")\n",
        "    lime_explain_sklearn_model(\n",
        "        explainer=lime_explainer,\n",
        "        model=tree_clf,\n",
        "        X_test=X_test,\n",
        "        instance_index=0,\n",
        "        num_features=10\n",
        "    )\n",
        "    # Random Forest\n",
        "    print(\"\\n==========================\")\n",
        "    print(\"MODELO 3: RANDOM FOREST\")\n",
        "    print(\"==========================\")\n",
        "\n",
        "    # Entrenar Random Forest\n",
        "    print(\"\\n[RF] Entrenando modelo...\")\n",
        "    rf_clf = random_forest(X_train, y_train, X_test, y_test)\n",
        "\n",
        "    # SHAP para Random Forest\n",
        "    print(\"\\n[RF] SHAP\")\n",
        "    shap_random_forest_fast(\n",
        "        rf_clf,\n",
        "        X_test,\n",
        "        feature_names=original_feature_names,\n",
        "        sample_size=300\n",
        "    )\n",
        "\n",
        "    # LIME para Random Forest\n",
        "    print(\"\\n[RF] LIME\")\n",
        "    lime_explain_sklearn_model(\n",
        "        explainer=lime_explainer,\n",
        "        model=rf_clf,\n",
        "        X_test=X_test,\n",
        "        instance_index=0,\n",
        "        num_features=10\n",
        "    )\n",
        "\n",
        "    print(f\"--- Permutation Importances Decision Tree---\\n\")\n",
        "    permutation_importance_analysis(tree_clf, X_test, y_test, feature_names=[\"product_A_sold_in_the_past\",\"product_B_sold_in_the_past\",\"product_A_recommended\",\"product_A\",\"product_C\",\"product_D\",\"cust_hitrate\",\"cust_interactions\",\"cust_contracts\",\"opp_month\",\"opp_old\",\"competitor_Z\",\"competitor_X\",\"competitor_Y\",\"cust_in_iberia\"])\n",
        "\n",
        "    print(f\"\\n--- Permutation Importances Random Forest---\\n\")\n",
        "    permutation_importance_analysis(rf_clf, X_test, y_test, feature_names=[\"product_A_sold_in_the_past\",\"product_B_sold_in_the_past\",\"product_A_recommended\",\"product_A\",\"product_C\",\"product_D\",\"cust_hitrate\",\"cust_interactions\",\"cust_contracts\",\"opp_month\",\"opp_old\",\"competitor_Z\",\"competitor_X\",\"competitor_Y\",\"cust_in_iberia\"])\n",
        "main()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}